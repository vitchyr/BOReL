import argparse
import datetime
import joblib
import os
import random
import numpy as np
import torch
from doodad.wrappers.easy_launch import save_doodad_config, DoodadConfig
from tqdm import tqdm
from utils import helpers as utl, offline_utils as off_utl
from utils import evaluation as utl_eval
import utils.config_utils as config_utl
from torchkit.pytorch_utils import set_gpu_mode
import matplotlib.pyplot as plt
from torchkit import pytorch_utils as ptu
from models.vae import VAE
from tensorboardX import SummaryWriter
import time
from vae_config import (
    args_gridworld, args_point_robot_sparse, args_cheetah_vel, args_ant_semicircle_sparse,
    args_hopper_param,
    args_humanoid_dir,
    args_walker_param,
)



BAR_LENGTH = 32     # for nicely printing training progress
NUM_EVAL_TASKS = 2


def vis_train_tasks(env, goals):
    env.plot_env()
    for goal in goals:
        circle = plt.Circle((goal[0], goal[1]), radius=env.goal_radius, alpha=0.3)
        plt.gca().add_artist(circle)


def eval_vae(dataset, vae, args):

    num_tasks = len(dataset)
    reward_preds = np.zeros((num_tasks, args.trajectory_len))
    rewards = np.zeros((num_tasks, args.trajectory_len))
    random_tasks = np.random.choice(len(dataset), NUM_EVAL_TASKS)  # which trajectory to evaluate

    for task_idx, task in enumerate(random_tasks):
        traj_idx_random = np.random.choice(dataset[task][0].shape[1])  # which trajectory to evaluate
        # traj_idx_random = np.random.choice(np.min([d[0].shape[1] for d in dataset]))
        # get prior parameters
        with torch.no_grad():
            task_sample, task_mean, task_logvar, hidden_state = vae.encoder.prior(batch_size=1)
        for step in range(args.trajectory_len):
            # update encoding
            task_sample, task_mean, task_logvar, hidden_state = utl.update_encoding(
                encoder=vae.encoder,
                obs=ptu.FloatTensor(dataset[task][3][step, traj_idx_random]).unsqueeze(0),
                action=ptu.FloatTensor(dataset[task][1][step, traj_idx_random]).unsqueeze(0),
                reward=ptu.FloatTensor(dataset[task][2][step, traj_idx_random]).unsqueeze(0),
                done=ptu.FloatTensor(dataset[task][4][step, traj_idx_random]).unsqueeze(0),
                hidden_state=hidden_state
            )

            rewards[task_idx, step] = dataset[task][2][step, traj_idx_random].item()
            reward_preds[task_idx, step] = ptu.get_numpy(
                vae.reward_decoder(task_sample.unsqueeze(0),
                                   ptu.FloatTensor(dataset[task][3][step, traj_idx_random]).unsqueeze(0).unsqueeze(0),
                                   ptu.FloatTensor(dataset[task][0][step, traj_idx_random]).unsqueeze(0).unsqueeze(0),
                                   ptu.FloatTensor(dataset[task][1][step, traj_idx_random]).unsqueeze(0).unsqueeze(0))[0, 0])

    return rewards, reward_preds


def update_step(vae, obs, actions, rewards, next_obs, args):
    episode_len, num_episodes, _ = obs.shape

    # get time-steps for ELBO computation
    if args.vae_batch_num_elbo_terms is not None:
        elbo_timesteps = np.stack(
            [np.random.choice(range(0, args.trajectory_len + 1), args.vae_batch_num_elbo_terms, replace=False)
             for _ in range(num_episodes)])
    else:
        elbo_timesteps = np.repeat(np.arange(0, args.trajectory_len + 1).reshape(1, -1),
                                   num_episodes, axis=0)

    # pass through encoder (outputs will be: (max_traj_len+1) x number of rollouts x latent_dim -- includes the prior!)
    _, latent_mean, latent_logvar, _ = vae.encoder(actions=actions,
                                                   states=next_obs,
                                                   rewards=rewards,
                                                   hidden_state=None,
                                                   return_prior=True)

    rew_recon_losses, state_recon_losses, task_recon_losses, kl_terms = [], [], [], []

    # for each task we have in our batch
    for episode_idx in range(num_episodes):

        # get the embedding values (size: traj_length+1 * latent_dim; the +1 is for the prior)
        curr_means = latent_mean[:episode_len + 1, episode_idx, :]
        curr_logvars = latent_logvar[:episode_len + 1, episode_idx, :]
        # take one sample for each ELBO term
        curr_samples = vae.encoder._sample_gaussian(curr_means, curr_logvars)

        # select data from current rollout (result is traj_length * obs_dim)
        curr_obs = obs[:, episode_idx, :]
        curr_next_obs = next_obs[:, episode_idx, :]
        curr_actions = actions[:, episode_idx, :]
        curr_rewards = rewards[:, episode_idx, :]

        num_latents = curr_samples.shape[0]  # includes the prior
        num_decodes = curr_obs.shape[0]

        # expand the latent to match the (x, y) pairs of the decoder
        dec_embedding = curr_samples.unsqueeze(0).expand((num_decodes, *curr_samples.shape)).transpose(1, 0)

        # expand the (x, y) pair of the encoder
        dec_obs = curr_obs.unsqueeze(0).expand((num_latents, *curr_obs.shape))
        dec_next_obs = curr_next_obs.unsqueeze(0).expand((num_latents, *curr_next_obs.shape))
        dec_actions = curr_actions.unsqueeze(0).expand((num_latents, *curr_actions.shape))
        dec_rewards = curr_rewards.unsqueeze(0).expand((num_latents, *curr_rewards.shape))

        if args.decode_reward:
            # compute reconstruction loss for this trajectory
            # (for each timestep that was encoded, decode everything and sum it up)
            rrl = vae.compute_rew_reconstruction_loss(dec_embedding, dec_obs, dec_next_obs,
                                                      dec_actions, dec_rewards)
            # sum along the trajectory which we decoded (sum in ELBO_t)
            if args.decode_only_past:
                curr_idx = 0
                past_reconstr_sum = []
                for i, idx_timestep in enumerate(elbo_timesteps[episode_idx]):
                    dec_until = idx_timestep
                    if dec_until != 0:
                        past_reconstr_sum.append(rrl[curr_idx:curr_idx + dec_until].sum())
                    curr_idx += dec_until
                rrl = torch.stack(past_reconstr_sum)
            else:
                rrl = rrl.sum(dim=1)
            rew_recon_losses.append(rrl)
        if args.decode_state:
            srl = vae.compute_state_reconstruction_loss(dec_embedding, dec_obs, dec_next_obs, dec_actions)
            srl = srl.sum(dim=1)
            state_recon_losses.append(srl)
        if not args.disable_stochasticity_in_latent:
            # compute the KL term for each ELBO term of the current trajectory
            kl = vae.compute_kl_loss(curr_means, curr_logvars, elbo_timesteps[episode_idx])
            kl_terms.append(kl)

    # sum the ELBO_t terms per task
    if args.decode_reward:
        rew_recon_losses = torch.stack(rew_recon_losses)
        rew_recon_losses = rew_recon_losses.sum(dim=1)
    else:
        rew_recon_losses = ptu.zeros(1)  # 0 -- but with option of .mean()

    if args.decode_state:
        state_recon_losses = torch.stack(state_recon_losses)
        state_recon_losses = state_recon_losses.sum(dim=1)
    else:
        state_recon_losses = ptu.zeros(1)

    if not args.disable_stochasticity_in_latent:
        kl_terms = torch.stack(kl_terms)
        kl_terms = kl_terms.sum(dim=1)
    else:
        kl_terms = ptu.zeros(1)

    # make sure we can compute gradients
    if not args.disable_stochasticity_in_latent:
        assert kl_terms.requires_grad
    if args.decode_reward:
        assert rew_recon_losses.requires_grad
    if args.decode_state:
        assert state_recon_losses.requires_grad

    return rew_recon_losses.mean(), state_recon_losses.mean(), kl_terms.mean()


def train(vae, dataset, args):
    '''

    :param vae:
    :param dataset: list of lists. each list for different task contains torch tensors of s,a,r,s',t
    :param args:
    :return:
    '''

    if args.log_tensorboard:
        writer = SummaryWriter(args.full_save_path)

    num_tasks = len(dataset)

    start_time = time.time()
    total_updates = 0
    for iter_ in range(args.num_iters):
        n_batches = np.min([
            int(np.ceil(d[0].shape[1] / args.vae_batch_num_rollouts_per_task))
            for d in dataset
        ])
        # traj_permutation = np.random.permutation(dataset[0][0].shape[1])
        traj_permutation = np.random.permutation(np.min([d[0].shape[1] for d in dataset]))
        loss_tr, rew_loss_tr, state_loss_tr, kl_loss_tr = 0, 0, 0, 0  # initialize loss for epoch
        n_updates = 0   # count number of updates
        for i in tqdm(range(n_batches), desc="Epoch {}".format(iter_)):

            if i == n_batches - 1:
                traj_indices = traj_permutation[i * args.vae_batch_num_rollouts_per_task:]
            else:
                traj_indices = traj_permutation[i * args.vae_batch_num_rollouts_per_task:
                                                (i + 1) * args.vae_batch_num_rollouts_per_task]

            n_task_batches = int(np.ceil(num_tasks / args.tasks_batch_size))
            task_permutation = np.random.permutation(num_tasks)

            for j in range(n_task_batches):     # run over tasks
                if j == n_task_batches - 1:
                    indices = task_permutation[j * args.tasks_batch_size:]
                else:
                    indices = task_permutation[j * args.tasks_batch_size:(j + 1) * args.tasks_batch_size]

                obs, actions, rewards, next_obs = [], [], [], []
                for idx in indices:
                    # random_subset = np.random.permutation(dataset[idx][0].shape[1], )
                    # random_subset = np.random.choice(dataset[idx][0].shape[1], args.vae_batch_num_rollouts_per_task)
                    obs.append(ptu.FloatTensor(dataset[idx][0][:, traj_indices, :]))
                    actions.append(ptu.FloatTensor(dataset[idx][1][:, traj_indices, :]))
                    rewards.append(ptu.FloatTensor(dataset[idx][2][:, traj_indices, :]))
                    next_obs.append(ptu.FloatTensor(dataset[idx][3][:, traj_indices, :]))
                obs = torch.cat(obs, dim=1)
                actions = torch.cat(actions, dim=1)
                rewards = torch.cat(rewards, dim=1)
                next_obs = torch.cat(next_obs, dim=1)
                rew_recon_loss, state_recon_loss, kl_term = update_step(vae, obs, actions, rewards, next_obs, args)

                # take average (this is the expectation over p(M))
                loss = args.rew_loss_coeff * rew_recon_loss + \
                       args.state_loss_coeff * state_recon_loss + \
                       args.kl_weight * kl_term
                # update
                vae.optimizer.zero_grad()
                loss.backward()
                vae.optimizer.step()

                n_updates += 1
                loss_tr += loss.item()
                rew_loss_tr += rew_recon_loss.item()
                state_loss_tr += state_recon_loss.item()
                kl_loss_tr += kl_term.item()

        print('Elapsed time: {:.2f}, loss: {:.4f} -- rew_loss: {:.4f} -- state_loss: {:.4f} -- kl: {:.4f}'
              .format(time.time() - start_time, loss_tr / n_updates, rew_loss_tr / n_updates,
                      state_loss_tr / n_updates, kl_loss_tr / n_updates))

        total_updates += n_updates
        # log tb
        if args.log_tensorboard:
            writer.add_scalar('loss/vae_loss', loss_tr / n_updates, total_updates)
            writer.add_scalar('loss/rew_recon_loss', rew_loss_tr / n_updates, total_updates)
            writer.add_scalar('loss/state_recon_loss', state_loss_tr / n_updates, total_updates)
            writer.add_scalar('loss/kl', kl_loss_tr / n_updates, total_updates)
            if args.env_name != 'GridNavi-v2':  # TODO: eval for gridworld domain
                rewards_eval, reward_preds_eval = eval_vae(dataset, vae, args)
                for task in range(NUM_EVAL_TASKS):
                    writer.add_figure('reward_prediction/task_{}'.format(task),
                                      utl_eval.plot_rew_pred_vs_rew(rewards_eval[task, :],
                                                                    reward_preds_eval[task, :]),
                                      total_updates)

        if (iter_ + 1) % args.eval_interval == 0:
            pass

        if args.save_model and (iter_ + 1) % args.save_interval == 0:
            save_path = os.path.join(os.getcwd(), args.full_save_path, 'models')
            if not os.path.exists(save_path):
                os.mkdir(save_path)
            torch.save(vae.encoder.state_dict(),
                       os.path.join(save_path, "encoder{0}.pt".format(iter_ + 1)))
            if vae.reward_decoder is not None:
                torch.save(vae.reward_decoder.state_dict(),
                           os.path.join(save_path, "reward_decoder{0}.pt".format(iter_ + 1)))
            if vae.state_decoder is not None:
                torch.save(vae.state_decoder.state_dict(),
                           os.path.join(save_path, "state_decoder{0}.pt".format(iter_ + 1)))


def _train_vae(
        log_dir,
        offline_buffer_path,
        saved_tasks_path,
        env_type,
        seed,
        path_length,
        meta_episode_len,
        load_buffer_kwargs=None,
        **kwargs
):
    if load_buffer_kwargs is None:
        load_buffer_kwargs = {}
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)

    parser = argparse.ArgumentParser()
    # parser.add_argument('--env-type', default='gridworld')
    # parser.add_argument('--env-type', default='point_robot_sparse')
    # parser.add_argument('--env-type', default='cheetah_vel')
    parser.add_argument('--env-type', default='ant_semicircle_sparse')
    extra_args = []
    for k, v in kwargs.items():
        extra_args.append('--{}'.format(k))
        extra_args.append(str(v))
    args, rest_args = parser.parse_known_args(args=extra_args)

    # --- GridWorld ---
    if env_type == 'cheetah_vel':
        args = args_cheetah_vel.get_args(rest_args)
        args.env_name = 'HalfCheetahVel-v0'
    elif env_type == 'ant_dir':
        # TODO: replace with ant_dir env
        args = args_ant_semicircle_sparse.get_args(rest_args)
        parser.add_argument('--env-name', default='AntSemiCircleSparse-v0')
        args.env_name = 'AntDir-v0'
    elif env_type == 'walker':
        args = args_walker_param.get_args(rest_args)
    elif env_type == 'hopper':
        args = args_hopper_param.get_args(rest_args)
    elif env_type == 'humanoid':
        args = args_humanoid_dir.get_args(rest_args)
    else:
        import ipdb; ipdb.set_trace()

    set_gpu_mode(torch.cuda.is_available() and args.use_gpu)

    args, env = off_utl.expand_args(args)
    args.save_dir = os.path.join(log_dir, 'trained_vae')

    args.trajectory_len = path_length
    task_data = joblib.load(saved_tasks_path)
    tasks = task_data['tasks']
    dataset, goals = off_utl.load_pearl_buffer(
        pretrain_buffer_path=offline_buffer_path,
        tasks=tasks,
        add_done_info=env.add_done_info,
        path_length=path_length,
        meta_episode_len=meta_episode_len,
        **load_buffer_kwargs
    )
    for data in dataset:
        print(data[0].shape)

    dataset = [[x.astype(np.float32) for x in d] for d in dataset]

    if args.save_model:
        dir_prefix = args.save_dir_prefix if hasattr(args, 'save_dir_prefix') \
                                             and args.save_dir_prefix is not None else ''
        args.full_save_path = os.path.join(args.save_dir, args.env_name,
                                           dir_prefix + datetime.datetime.now().strftime('__%d_%m_%H_%M_%S'))
        os.makedirs(args.full_save_path, exist_ok=True)
        config_utl.save_config_file(args, args.full_save_path)

    vae = VAE(args)
    train(vae, dataset, args)


def train_vae(doodad_config: DoodadConfig, params):
    save_doodad_config(doodad_config)
    _train_vae(doodad_config.output_directory, **params)
